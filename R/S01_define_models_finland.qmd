---
title: "Quantitative biodiversity modelling for finance and companies"
subtitle: "Project 1: compare metrics"
author: 
  - name: "Carlos Muñoz"
    email: carlos_jair.munoz@cec.lu.se
    affiliations: 
      - ref: CEC

affiliations:
  - id: CEC
    name: Centre for Environmental and Climate Science, Lund University
    department: Centre for Environmental and Climate Science 
    address: Kontaktvägen 10
    city: Lund
    postal-code: 22362
    url: https://www.cec.lu.se/home
   
format:
  html:
    theme: cosmo # try other themes from https://quarto.org/docs/output-formats/html-themes.html
    toc: true # this will enable Table of Contents
    code-fold: false #try changing it and see what happens
    embed-resources: true # this is so your file would embed all images. 
    output-dir: "../quarto_reports/"  
  pdf: default
  docx: default

execute: 
  echo: true    #use to show/hide R-code in the report
  cache: false
number-sections: true #sectioner can be automatically numbered
title-block-banner: true
---

## Introduction

### General framework

-   Biodiversity is declining at an alarming rate, faster than ever in human history (Barnosky et al., 2011; Ceballos et al., 2015).
-   Economic activity is at the core of the biodiversity crisis, but biodiversity loss also represents a significant physical threat, reputational and regulatory risk to business and financial activities.
-   BIOPATH (Sweden) seeks to develop innovative approaches to integrate biodiversity considerations into financial and business decision-making to halt and reverse biodiversity loss.

### Biodiversity metrics

-   Biodiversity databases are the foundation for effective management strategies, but data alone isn't enough to stop biodiversity loss. We need to turn this data into useful information. This means using some way of biodiversity measurement to move activities from harming nature to being neutral or even positive.
-   Burgess and colleagues (2024) identified 573 metrics developed to inform decision-making related to biodiversity.
-   What metrics should be used?

#### A Tension

-   Business Perspective: Need for accessible, cost-effective and standardized. Importance of simplicity.
-   Natural scientific Perspective:Reflect ecological complexity.Capture multiple dimensions.
-   Maybe a set instead of only one silver bullet

### Challenge

-   The heart of the matter with biodiversity state metrics is how biodiversity change is interpreted and, consequently, how choices are made may be significantly impacted by the selection and response of biodiversity measures (Silvestro et al., 2023). It is critical to understand how different measures respond to changes, which metrics are particularly sensitive to early signals of biodiversity loss, and which metrics respond to changes consistently.
-   How transparently, efficiently, and credibly these measures capture the state and changes on multiple dimensions of biodiversity?

### Objective

To compare biodiversity metrics using quantitative models to assess how accurately they reflect biodiversity states and trends in different biodiversity change scenarios.

-   Birds as focal study taxon
-   Biodiversity change because change of habitat: forestry management scenarios

**THERE IS A NOTORIOUS GAP ABOUT WHY TO USE BIRDS AND WHY FOREST**

### Rol of Hmsc

An Hmsc model provides predictions at the species-community level. This enables the spatial representation of biodiversity to analyse and evaluate biodiversity metrics’ performance and variation through environmental and spatial gradients while counting for uncertainties.

## Setup

### Environment Configuration

```{r, setup}
knitr::opts_knit$set(root.dir = "c:/Users/Carlos Munoz/Documents/Ph.D/6_courses/2025_I_comparative_metrics/")
```

### Load Libraries

```{r}
#| message: false
#| warning: false

library(matrixStats)
library(Hmsc)
library(tidyverse)
library(data.table)
library(terra)
library(sf)
library(maps)
library(ape)
library(taxize)
library(remotes)
library(phytools)
library(openxlsx)
library(tidyterra)
library(viridis)
library(lwgeom)
library(ggrepel)
library(gt)
library(exactextractr)
library(patchwork)
library(dplyr)
library(units)
library(clootl) # devtools::install_github("eliotmiller/clootl")
library(finbif) # https://github.com/luomus/finbif; usethis::edit_r_environ()
```

### Global Options & Helper Functions

```{r}
options("GDAL_DATA" = Sys.getenv("GDAL_DATA")) # Ensure GDAL_DATA is set if needed
options("OSR_USE_NON_DEPRECATED" = "NO")
```

```{r}
set.seed(11072024)
```


```{r}
transform_Y_to_abunCpres <- function(data) {
  data |> 
    mutate(across(everything(), ~ ifelse(. == 0, NA, scale(log(.)))))
}
```

```{r}
source("R/_utilities_transform_covariates.R")
```

```{r}
options(timeout = 9999)
```

## Data loading

### Load Finnish Bird Survey (FBS) Data


```{r}
# coordinates 
coords <- st_read("data/fbs/vakiolinja/Vakiolinjat_routes.geojson") |> 
  st_transform(2393)

# routes (names and dates)
routes <- read.csv("data/fbs/vakiolinja/vakiolinja1_20250626.csv")

# occurrences by route (spp key, and counts in points and lines)

occurr_route <- read.csv("data/fbs/vakiolinja/vakiolinja2_20250626.csv") 

# occurrences by section route (spp key, and counts in points and lines)

occurr_sect_route <- read.csv("data/fbs/vakiolinja/vakiolinja3_20250626.csv") 

```

### Load Auxiliary Files

Species list from Bakx et al, 2023.

```{r}
bakx_species <- read.xlsx(xlsxFile = "data/ecs24559-sup-0001-appendixs1.xlsx", sheet = 1)
```

Traits information
```{r}
avonet <- read.xlsx("data/traits/AVONET/TraitData/AVONET2_eBird.xlsx", sheet = 2)
```

Dictionary of processed covariates (Shultz)
```{r}
dict_covar <- read.csv("data/covariates/dictionary_covariates.csv", sep = ";")
```

Multi Source Forest National Inventory available data years

```{r}
ms_nfi_years <- c(2009, 2011, 2013, 2015, 2017, 2019, 2021)
```

Phylogenetic information

```{r}
run <- F
if(run){
  get_avesdata_repo(path="data/") # 1GB more or less
}
set_avesdata_repo_path(path = "data/AvesData-main/", overwrite = T)

```
```{r}
run <- F
if(run){
  download.file("https://www.avilist.org/wp-content/uploads/2025/06/AviList-v2025-11Jun-extended.xlsx", destfile = "data/AviList-v2025-11Jun-extended.xlsx", mode = "wb")  
}
avilist <- read.xlsx("data/AviList-v2025-11Jun-extended.xlsx", sheet = 1)
```

## FBS Data Preprocessing & Cleaning


```{r}
# Extracting and transforming route number information in order to use it as an easier index

coords_nm <- str_split(coords$name, pattern = ", ", simplify = T) |> 
  as.data.frame()
names(coords_nm) <- c("vakio", "str1", "str2", "numer")

coords <- bind_cols(coords, coords_nm) |>
  mutate(vakio = as.numeric(vakio), spatial_length = drop_units(st_length(coords))) |> 
  arrange(vakio)

routes <-  routes |> 
  mutate(vakio = as.numeric(vakio))
occurr_route <- occurr_route |> 
  mutate(vakio = as.numeric(vakio))
occurr_sect_route <- occurr_sect_route |> 
  mutate(vakio = as.numeric(vakio))
  
```

### Species information retrieving

Species information in the FBS routes. There is no taxonomic data, so it needs to be retrieved from finbif.

```{r}
spp_in_route <- occurr_sect_route |> 
  group_by(scientificName) |> 
  summarise(count = n(), .groups = "drop") |> 
  filter(scientificName != "") 

run <-  T

if(run == T){

  finbif_info_raw <- list() 
  
  spp <- spp_in_route |> 
    pull(scientificName) 
  
  for(i in 1:nrow(spp_in_route)){
    #i <- 1
    spp.i <- spp[i]
    resp.i <- tryCatch(
      {
        finbif::finbif_taxa(spp.i, type = "exact", cache = getOption("finbif_use_cache"))$content[[1]]
      },
      error = function(e) {
        finbif::finbif_taxa(spp.i, type = "partial", cache = getOption("finbif_use_cache"))$content[[1]]
      }
    )
    
    if(is.null(resp.i$vernacularName$en)) resp.i$vernacularName$en <- NA
    
    finbif_info_raw[[i]] <- data.frame("taxonId" = resp.i$id, "acceptedName_finbif" = resp.i$scientificName, "englishName" = resp.i$vernacularName$en,
                                        "taxonRank" = resp.i$taxonRank, "is.species" = resp.i$species, "group" = resp.i$informalGroups[[1]]$name$en) 
  }
  
  finbif_info_raw <- finbif_info_raw |> 
    bind_rows() |> 
    mutate(count = spp_in_route$count)
  
  saveRDS(object = finbif_info_raw, file = "data/fbs/tax_raw_data.rds")

  rm(spp.i, resp.i, i)
  
}else{
  
  finbif_info_raw <- readRDS(file = "data/fbs/tax_raw_data.rds")
  
}

finbif_info <- finbif_info_raw |>
  mutate(acceptedName_finbif = ifelse(taxonRank == "MX.subspecies",
                                        stringr::word(acceptedName_finbif, 1, 2),
                                        acceptedName_finbif
                                        )
         )

occurr_sect_route <- occurr_sect_route |> 
  left_join(finbif_info, by = "taxonId")
```

### Resolving bird names to international standard


```{r}

route_species <- finbif_info |> 
  filter(is.species == TRUE, group == "Birds", taxonRank != "MX.hybrid", taxonRank != "MX.form")

run <- F
if(run){
  itis_routes_raw <- lapply(route_species$acceptedName_finbif, function(species_name) {
  tryCatch(
    {
      tsn <- taxize::get_tsn(species_name, accepted = FALSE)
      accepted_info <- taxize::itis_acceptname(tsn)
      accepted_info$submittedname <- species_name
      
      return(accepted_info)
    },
    error = function(e) {
      return(
        data.frame(submittedtsn = NA, acceptedname = NA, acceptedtsn = NA, author = NA, submittedname = species_name)
        )
      }
    )
  })
  itis_routes_raw <- dplyr::bind_rows(itis_routes_raw)
  saveRDS(itis_routes_raw, file.path("data", "fbs", "itis_routes_raw.rds"))
}else{
  itis_routes_raw <- readRDS(file.path("data", "fbs", "itis_routes_raw.rds"))
}

itis_routes <- itis_routes_raw |> 
  bind_cols(route_species) |> 
  mutate(acceptedname_itis = ifelse(is.na(acceptedname),
                               acceptedName_finbif,
                               acceptedname
                               ),
         acceptedname_itis = stringr::word(acceptedname_itis, 1, 2) # in itis some accepted names are subspecies
         ) |> 
  select(taxonId, submittedname, acceptedname_itis)
  
```


```{r}

run <- F
if(run){
  itis_bakx_raw <- lapply(bakx_species$Latin_name, function(species_name) {
  tryCatch(
    {
      tsn <- taxize::get_tsn(species_name, accepted = FALSE)
      accepted_info <- taxize::itis_acceptname(tsn)
      accepted_info$submittedname <- species_name
      
      return(accepted_info)
    },
    error = function(e) {
      return(
        data.frame(submittedtsn = NA, acceptedname = NA, acceptedtsn = NA, author = NA, submittedname = species_name)
        )
      }
    )
  })
  itis_bakx_raw <- dplyr::bind_rows(itis_bakx_raw)
  saveRDS(itis_bakx_raw, file.path("data", "fbs", "itis_bakx_raw.rds"))
}else{
  itis_bakx_raw <- readRDS(file.path("data", "fbs", "itis_bakx_raw.rds"))
}

itis_bakx <- itis_bakx_raw |> 
  mutate(acceptedname_itis = ifelse(is.na(acceptedname),
                               submittedname,
                               acceptedname
                               )
         ) |> 
  select(submittedname, acceptedname_itis)

```

```{r}
harmonized_species <- full_join(
  itis_routes, 
  itis_bakx, 
  by = "acceptedname_itis", 
  suffix = c("_routes", "_bakx")
)

harmonized_species <- harmonized_species |>
  mutate(
    in_routes = !is.na(submittedname_routes),
    in_bakx = !is.na(submittedname_bakx),
    acceptedName = acceptedname_itis
  ) |>
  select(
    taxonId,
    submittedname_bakx,
    acceptedName,
    in_routes,
    in_bakx
  )

```

Once harmonized both datasets using finbif we found that 

```{r}
table("routes" = harmonized_species$in_routes, "bakx" = harmonized_species$in_bakx)
```
We are missing the next birds from Bakx et al (2023):

```{r}
harmonized_species$submittedname_bakx[!harmonized_species$in_routes]

```


```{r}
occurr_sect_route <- occurr_sect_route |> 
  left_join(harmonized_species, by = "taxonId") 
  
```


## Exploratory Data Analysis & Quality Checks


### Create Master route Table

  filter(is.species == TRUE, group == "Birds", taxonRank != "MX.hybrid", taxonRank != "MX.form") |>

```{r}

master_routes <- routes |>
  # FLAG 1: Temporal Filter ---
  # Time window: late Spring/Summer routes of selected years ( because data availability of the multi source national forest inventory, MS-NFI)
  # Check vents outside the desired time window.
  mutate(
    is_valid_date = (month >= 5 & month <= 7 & year %in% ms_nfi_years) 
  ) |>
  # FLAG 2: Species Filter ---
  # Check which routes contain at least one observation of a target species.
  left_join(
    occurr_sect_route |>
      filter(in_bakx == TRUE) |> # this flag sums is.species, group == birds, in_bakx
      distinct(vakio) |>
      mutate(has_target_species = TRUE),
    by = "vakio"
  ) |>
  #
  # FLAG 3: Biotope Data Quality ---
  # Check for routes that are missing biotope information in any given year.
  #
  left_join(
    occurr_sect_route |>
      group_by(year, vakio) |>
      summarise(
        sections_missing_biotope = sum(biotope == ""),
        total_sections = n(),
        .groups = 'drop'
      ) |>
      # A route is invalid if all its sections are missing biotope info
      mutate(has_no_biotope_data = (sections_missing_biotope == total_sections)) |>
      filter(has_no_biotope_data) |>
      distinct(vakio, .keep_all = TRUE), # Keep only the invalid routes to flag them
    by = c("vakio", "year")
  ) |>
  #
  # FLAG 4: Spatial Length Discrepancy ---
  # Check if the recorded line length differs from the spatial object length.
  left_join(coords, by = "vakio") |>
  mutate(
    length_difference = abs(linelength - spatial_length),
    is_length_discrepant = (length_difference > 50)
  ) |>
  #
  mutate(
    has_target_species = !is.na(has_target_species),
    has_no_biotope_data = !is.na(has_no_biotope_data)
  )
```

### Exploratory Analysis of Filter Criteria

How much data is affected by missing biotope information?

```{r}
# Summarize how many years each route is missing all biotope data
biotope_missing_summary <- master_routes |>
  filter(has_no_biotope_data == TRUE) |>
  count(vakio, year, name = "times_missing")

# Join this summary to the spatial data for mapping
coords_for_plot <- coords |>
  left_join(biotope_missing_summary, by = "vakio") |>
  # If a route never had missing data, times_missing will be NA. We set it to 0.
  mutate(times_missing = as.factor(ifelse(is.na(times_missing), 0, times_missing)))

# What percentage of routes have this issue at least once?
# And how many times have they been without biotope info?
n_routes_total <- n_distinct(master_routes$vakio)
n_routes_with_missing_data <- n_distinct(biotope_missing_summary$vakio)
percent_affected <- round((n_routes_with_missing_data / n_routes_total) * 100, 2)

print(paste0(percent_affected, "% of routes have at least one year with no biotope data."))

# Display the frequency table
table(coords_for_plot$times_missing) |>
  as.data.frame() |>
  rename(
    `Years of Missing Biotope Data` = Var1,
    `Number of Routes` = Freq
  ) |>
  gt()
```

How are the routes with missing biotope data distributed spatially?

```{r}
# mapping spreading of no biotope data
fin <- st_as_sf(maps::map(database = "world", regions = "finland", plot = FALSE, fill = TRUE))

# Create a color palette
lvls <- levels(coords_for_plot$times_missing)
turbo_discrete_colors <- viridis_pal(option = "turbo")(n = length(lvls) - 1)
custom_colors <- c(
  "0" = "gray",
  setNames(turbo_discrete_colors, lvls[lvls != "0"])
)

# Plot the map
coords_for_plot |>
  ggplot() +
  geom_sf(data = fin) +
  geom_sf(aes(color = times_missing), size = 1.5) +
  scale_color_manual(
    values = custom_colors,
    name = "Number of Years Missing Biotope Data",
    drop = FALSE
  ) +
  labs(
    title = "Frequency of Years with Missing Biotope Data per Survey Route",
    subtitle = "Routes with no missing data are shown in grey."
  ) +
  theme_minimal()
```
How many routes are missing data each year?

```{r}
missing_by_year <- master_routes |>
  group_by(year) |>
  summarise(
    # Count of routes missing data that year
    n_routes_missing_biotope_data = sum(has_no_biotope_data),
    # Total routes sampled that year
    n_routes_sampled = n(),
    # Percentage of routes missing data that year
    percent_missing = (n_routes_missing_biotope_data / n_routes_sampled) * 100
  )

# Breakdown of missing biotope data by year
missing_by_year |> 
  gt()

```

What is the spatial distribution of routes with target species?

```{r}
# Summarize at the route level: does a route EVER have a target species?
species_summary <- master_routes |>
  group_by(vakio) |>
  summarise(has_target_species_ever = any(has_target_species))

# Join to spatial data for plotting
coords_for_species_plot <- coords |>
  left_join(species_summary, by = "vakio")

# Plot the map
coords_for_species_plot |>
  ggplot() +
  geom_sf(data = fin) +
  geom_sf(aes(color = has_target_species_ever), size = 1.5, alpha = 0.8) +
  scale_color_manual(
    values = c("TRUE" = "gray", "FALSE" = "red"),
    labels = c("TRUE" = "Species Information", "FALSE" = "No Species Information"),
    name = "Data Availability",
  ) +
  labs(
    title = "Spatial Distribution of Finnish Bird Survey Routes",
    subtitle = "Highlighting routes with and without focal species data"
  ) +
  theme_minimal()
```
If we remove routes with no biotope data, how much information do we lose of the target species?

```{r}
# Get the list of eventIDs that will be removed due to the biotope filter
events_to_remove <- master_routes |>
  filter(has_no_biotope_data == TRUE) |>
  pull(eventId) |>
  unique()

# Flag occurrence data
loss_analysis_data <- occurr_sect_route |> 
  filter(in_bakx == T) |>
  mutate(
    # Flag records based on whether their eventID is in the removal list
    biotope_logic = ifelse(eventId %in% events_to_remove, 0, 1)
  )

# --- summary ---
occurrence_data <-  table(acceptedName = loss_analysis_data$acceptedName,
                          biotope_logic = loss_analysis_data$biotope_logic) |>
  as.data.frame() |>
  pivot_wider(id_cols = acceptedName,
              names_from = biotope_logic,
              values_from = Freq,
              names_prefix = "flag_") |>
  rename(occurrences_lost = flag_0,
         occurrences_kept = flag_1)

paircount_data <- loss_analysis_data |>
  group_by(acceptedName, biotope_logic) |>
  summarise(total_pairCount = sum(pairCount, na.rm = TRUE), .groups = 'drop') |>
  pivot_wider(
    id_cols = acceptedName,
    names_from = biotope_logic,
    values_from = total_pairCount,
    values_fill = 0,
    names_prefix = "flag_"
  ) |>
  rename(paircount_lost = flag_0,
         paircount_kept = flag_1)
```


```{r}
#| label: data_loss_count
#| tbl-cap: data loss count
# Join and display the final table
left_join(occurrence_data, paircount_data, by = "acceptedName") |>
  mutate(
    total_occurrences = occurrences_lost + occurrences_kept,
    percent_occurrences_lost = round((occurrences_lost / total_occurrences) * 100, 2),
    total_paircount = paircount_lost + paircount_kept,
    percent_paircount_lost = round((paircount_lost / total_paircount) * 100, 2)
  ) |>
  arrange(desc(percent_paircount_lost)) |>
  gt() |>
    tab_header(title = "Potential Data Loss from Removing Routes with No Biotope Info")
  


```


### Apply Filter
```{r}
valid_events <- master_routes |>
  filter(
    is_valid_date == TRUE,
    has_target_species == TRUE,
    has_no_biotope_data == FALSE,
    is_length_discrepant == FALSE
  )

# Extract the unique IDs for the valid events and routes
valid_event_ids <- unique(valid_events$eventId)
valid_vakio_ids <- unique(valid_events$vakio)

#
# --- DATA CLEANING ---

coords <- coords |>
  filter(vakio %in% valid_vakio_ids)

routes <- routes |>
  filter(eventId %in% valid_event_ids)

occurr_route <- occurr_route |>
  filter(eventId %in% valid_event_ids)

occurr_sect_route <- occurr_sect_route |>
  filter(
    eventId %in% valid_event_ids,
    in_bakx == T
  )
```


```{r}
# Plot the map
coords |>
  ggplot() +
  geom_sf(data = fin) +
  geom_sf()
```

#### Species analysis


What is the distribution of bird orders among the target species after filtering?

```{r}
# # The code is the same, but the input is now correctly focused
# aves_order_data_barplot <- occurr_sect_route |> 
#   select(scientificName) |> 
#   distinct() |> 
#   left_join(target_spp_in_route, by = "scientificName") |>
#   filter(class == 'Aves') |>
#   dplyr::count(order, name = 'n') |>
#   arrange(desc(n))
# 
# ggplot(aves_order_data_barplot, aes(x = reorder(order, -n), y = n, fill = order)) +
#   geom_col(color = 'black', show.legend = FALSE) +
#   labs(title = 'Distribution of Orders Among Target Bird Species',
#        x = 'Order',
#        y = 'Number of Species') +
#   theme_minimal() +
#   theme(axis.text.x = element_text(angle = 45, hjust = 1),
#         plot.title = element_text(hjust = 0.5, size = 14, face = 'bold')) +
#   geom_text(aes(label = n), vjust = -0.5, size = 3.5)
```

Which Target Species Were Lost After Filtering?

```{r}
# Get a unique list of species present in the final, filtered dataset
species_in_final_dataset <- occurr_sect_route |> 
  pull(acceptedName) |> 
  unique()

target_species <- harmonized_species |> 
  filter(in_routes == T, in_bakx == T) |> 
  pull(acceptedName)

# Compare original target list ('species') with the final list
lost_species <- setdiff(target_species, species_in_final_dataset)

# Display the result in a table
if (length(lost_species) > 0) {
  tibble(Lost_Target_Species = lost_species) |>
    gt() |>
    tab_header(
      title = "Target Species Removed by Data Quality Filters"
    )
}
```


## Feature engineer

### Create Biotope Polygon Buffers

```{r}

run <- F # want to recalculate buffers?
if(run){
  
  biotope_polygon_L3_log <- tidyr::expand_grid(
  vakio = coords$vakio,
  year = ms_nfi_years
  ) |>
  mutate(
    path = NA_character_,
    was_processed = FALSE,
    was_extended = FALSE,
    extension_distance_m = NA_real_,
    point_warning = NA_character_
  ) |>
  arrange(year, vakio)

  for (y in 1:length(ms_nfi_years)) {
    #y <- 7
    current_year <- ms_nfi_years[y]
    message(paste("--- Processing year:", current_year, "---"))
  
    all_buffers_for_year <- list()
  
    for (a in 1:nrow(coords)) {
      # 
      route_id <- coords$vakio[a]
      
      log_row_index <- which(biotope_polygon_L3_log$vakio == route_id & biotope_polygon_L3_log$year == current_year)
      
      route_a <- coords |>
        filter(vakio == route_id) |> 
        # this solve spicules and a lot of problems on the topology of the line.
        # However some lines become a bit longer, this is why in some special cases
        # we extend the line to match with the maximum claimed distance by the surveyor
        st_simplify(preserveTopology = T, dTolerance = 20)
  
      biotope_route_a <- occurr_sect_route |>
        filter(year == current_year, vakio == route_id) |>
        select(vakio, biotopeStripId, startMeters, endMeters, meters, biotope, biotopeSpecifier) |>
        distinct() |> 
        arrange(startMeters) |> 
        mutate(
          r_startMeters = startMeters / as.numeric(st_length(route_a)),
          r_endMeters = endMeters / as.numeric(st_length(route_a))
        )
      
      # to extend sample lines
      if(any(biotope_route_a$r_startMeters > 1)){
        d <- (biotope_route_a$r_endMeters[nrow(biotope_route_a)] - 1) * as.numeric(st_length(route_a))
        route_a <- st_extend_line(line = route_a, distance = d, end = "end")
        
        biotope_route_a <- biotope_route_a |>
          mutate(
            r_startMeters = startMeters / as.numeric(st_length(route_a)),
            r_endMeters = endMeters / as.numeric(st_length(route_a))
          )
        
        biotope_polygon_L3_log$was_extended[log_row_index] <- TRUE
        biotope_polygon_L3_log$extension_distance_m[log_row_index] <- d
      
      }else{
        biotope_polygon_L3_log$was_extended[log_row_index] <- FALSE
      }
      
      if (nrow(biotope_route_a) > 1) {
        
        biotope_polygon_L3_log$was_processed[log_row_index] <- TRUE
        
        route_buffers <- list()
        for (r in 1:nrow(biotope_route_a)) {
          # r <- 30
          # some strips are actually points, here we spot it using starting and final distance information
          # then they are removed and flagged in the log file
          equal <- biotope_route_a[r,c("r_startMeters", "r_endMeters")] |> 
            unlist() |> 
            diff()
          
          if(equal != 0){ 
            
          # Create the line segment for this biotope section
          line_section <- st_linesubstring(
            route_a,
            from = biotope_route_a$r_startMeters[r],
            to = biotope_route_a$r_endMeters[r]
          )
  
          # A standard buffer on the raw, angular route lines can create unrealistic polygon artifacts around sharp and "spicule type" corners.
          # To solve this, we split each section and create sub-buffers and then join them.
          
          segments <- nngeo::st_segments(line_section, progress = F)
         
          segment_buffers <- st_buffer(segments, dist = 150, endCapStyle = "FLAT")
          
          final_buffer <- st_union(segment_buffers)
          
          final_buffer_sf <- st_sf(geometry = st_sfc(final_buffer), crs = st_crs(route_a)) |>
              bind_cols(biotope_route_a[r,])
  
          route_buffers[[r]] <- final_buffer_sf
          
          }else{
            
            value_in_log <- biotope_polygon_L3_log$point_warning[log_row_index]
            
            point_strip_r <-  biotope_route_a$biotopeStripId[r]
            
            warning(paste(point_strip_r, "route", route_id, "year", current_year, "is a point observation, check log."))
            
            value_to_log <- paste(c(value_in_log, point_strip_r), collapse = ",")
            
            biotope_polygon_L3_log$point_warning[[log_row_index]] <- value_to_log
            
          }
          
        }
        all_buffers_for_year[[a]] <- do.call(rbind, route_buffers)
      }
    }
  
    # Combine and save the final file for the year
    valid_buffers <- all_buffers_for_year[!sapply(all_buffers_for_year, is.null)]
    year_sf <- do.call(rbind, valid_buffers)
    year_sf$year <- current_year
  
    output_filename <- paste0("data/fbs/route_sections_L3_", current_year, ".gpkg")
    write_sf(year_sf, output_filename, delete_layer = TRUE)
    biotope_polygon_L3_log[biotope_polygon_L3_log$year == current_year, "path"] <- output_filename
    message(paste("Saved file:", output_filename))
  
  }
  saveRDS(biotope_polygon_L3_log, file.path("data", "fbs", "biotope_polygon_L3_log.rds"))  
  
  rm(line_section, segments, segment_buffers, final_buffer, final_buffer_sf, 
   route_buffers, all_buffers_for_year, valid_buffers, year_sf, output_filename)
  
}else{
  
  biotope_polygon_L3_log <- readRDS(file.path("data", "fbs", "biotope_polygon_L3_log.rds"))  
  
}

```

### Aggregate biotope information 

From vakio+biotope+biotopeSpecifier to vakio+biotope. The former is too finer, see @tbl-data_loss_count

#### Dissolve poligons

```{r}
l3_polygon_paths <- biotope_polygon_L3_log$path |> 
  unique()
l2_output_paths <- data.frame("year" = ms_nfi_years, "path_L2" = NA)

for (i in 1:length(l3_polygon_paths)) {
  # i <- 1
  l3_file <- l3_polygon_paths[i]
  current_year <- ms_nfi_years[i]
  message(paste("Processing:", basename(l3_file)))
  
  polygons_l3 <- st_read(l3_file, quiet = T)
  
  # Dissolve the geometries
  # We group by the route ID (vakio) and the Level 2 biotope code
  polygons_l2 <- polygons_l3 |>
    group_by(vakio, biotope) |>
    # st_union() merges all geometries within each group
    summarise(
      geometry = st_union(geom),
      dissolved_strip_ids = paste(unique(biotopeStripId), collapse = ","),
      num_dissolved = n(),
      year = unique(year),
      .groups = "drop"
    ) |> 
    mutate(
      sampleUnit = paste(vakio, year, biotope, sep = "_")
    )
  
  output_filename_l2 <- file.path("data", "fbs", paste0("route_sections_L2_", current_year, ".gpkg"))
  write_sf(polygons_l2, output_filename_l2, delete_layer = TRUE)
  l2_output_paths$path_L2[l2_output_paths$year == current_year] <- output_filename_l2
}
```

#### Sum up occurrences

```{r}
occurr_biotope_route <- occurr_sect_route |>
  group_by(vakio, year, biotope, acceptedName) |>
  summarise(
    pairCount = sum(pairCount, na.rm = TRUE),
    dissolved_strip_ids = paste(unique(biotopeStripId), collapse = ","),
    num_dissolved = n(),
    .groups = "drop"
  ) |>
  # We need to create a new unique ID for these aggregated units
  mutate(
    sampleUnit = paste(vakio, year, biotope, sep = "_")
  )
```


### Process Environmental Covariates

Online repositories are not deployed as Web Feature Service in which we can work "in-situ". It was needed to download the data sets. Data sets are massive and splitted over several storage units. To consult how they were downloaded please refer to scripts "_utilities_download_covariates.R" and "_utilities_download_covariates.py". Dowloaded files were processing following the "_utilities_fetch_covariates.R" script.

Raw data of each route with a 150m buffer and from metso/non metso stands is extracted with the script "_utilities_fetch_covariates.R". Once with this information 
we can compare each type of unit to inform potential risk of exrapolation. We sample 10000 stands for each type (metso/non metso) and all the route buffers.

```{r}

covar_nm <- dict_covar |> 
  pull(var_new) |> 
  unique()

year_route_samplingn <- occurr_biotope_route |> 
  group_by(sampleUnit) |>
  summarise(total_sections = n(), .groups = 'drop')

```

```{r}

folder_name <- file.path("data", "intermediate_covariates") 

run <- F

if(run){
  
  dir.create(folder_name, showWarnings = FALSE)
  output_rds_paths <- as.character()
  
  metso_stands_df <- st_read(file.path("data","metso", "treatment_control_stand.gpkg"), quiet = T) |> 
  st_drop_geometry() |> 
  select(standid, metso)
  
  sampled_stand_ids <- metso_stands_df |> 
    group_by(metso) |>
    sample_n(size = 10000, replace = FALSE) |>
    ungroup() |>
    pull(standid)
  
  rds_data_dir <- file.path("D:","intermediate_aggregated")

  rds_files <- list.files(rds_data_dir, pattern = "\\.rds$", recursive = TRUE, full.names = TRUE)

  for (i in 1:length(covar_nm)) {
    # i <- 1
    
    covariate <-  covar_nm[i]
    
    message(paste("--- Processing covariate:", covariate, "---"))
  
    filtered_data <- filter_covariate_data(
      covariate_name = covariate,
      files = rds_files,
      sampled_ids = sampled_stand_ids,
      route_map = year_route_sampling
    )
    
    if(is.null(filtered_data)){
      warning(paste("Covariate", covariate, "is null data"))
      next
    } 
  
    # writing a filtered data set just for coords because metso is a sample
    output_rds_path <- file.path(folder_name, paste0("filtered_cov_coords_", covariate, ".rds"))
    saveRDS(filter(filtered_data, polygon_source == "coords"), file = output_rds_path) 
    output_rds_paths[i] <- output_rds_path
    
    plot_data <- prepare_covariates_data_toplot(
      filtered_data = filtered_data,
      metso_map = metso_stands_df
    )
    rm(filtered_data);gc()
    
    is_binary <- dict_covar$binary[i]
    
    if (is_binary) {
      generate_binary_plot(covariate, plot_data, folder_name)
    } else {
      generate_covariate_plot(covariate, plot_data, folder_name)
    }
    rm(plot_data );gc()
  }
  
}else{
  output_rds_paths <- list.files(folder_name, pattern = ".rds$", recursive = T, full.names = T)
}

```

## Model Input Assembly

### Species Occurrence Matrix (Y)

```{r}
Y <- occurr_biotope_route |> 
  select(sampleUnit, acceptedName, pairCount) |> 
  pivot_wider(id_cols = sampleUnit, names_from = acceptedName, values_from = pairCount,
              values_fn = sum, values_fill = 0) |> 
  select(-sampleUnit) |> 
  as.data.frame()

```

```{r}
P <-  colMeans(Y > 0)
range(P) |> round(4)
hist(P, xlab = "Species prevalence (P)",xlim=c(0,1))
```

Most species exhibit low prevalence, clustering towards the rare end of the spectrum, while a few species show a significantly higher prevalence, indicating they are common across the sampled areas, even an species is occurring in the `r max(P)` of sites (`r names(which(P == max(P)))`).

### XData

Now we are going to derive polygon-Level statistics from the raw values of the covariates.  We use the covariate data dictionary to filter each variable as either continuous or binary. Next, the two variable types are handled differently: for each polygon and year, continuous variables are summarized with a weighted mean and standard deviation, min and max, while binary variables are summarized by calculating the weighted proportion of '1's. 

```{r}
covar_data <- purrr::map_dfr(.x = output_rds_paths, .f = ~ compact(readRDS(.x))) |>
  select(-c(poly_id, dataset, polygon_source)) 

binary_vars <- dict_covar |> 
  filter(binary == 1) |> 
  pull(var_new)

continuous_vars <- dict_covar|>
  filter(binary == 0)|>
  pull(var_new)

binary_summary <- covar_data|>
  filter(variable %in% binary_vars)|>
  group_by(polygon_id, year, variable)|>
  summarise(
    prop = weighted.mean(value, coverage_fraction, na.rm = TRUE),
    count = ceiling(sum(coverage_fraction[value == 1], na.rm = TRUE)),
    .groups = 'drop'
  ) |> 
  pivot_longer(cols = -c(polygon_id, year, variable), values_to = "value") |> 
  pivot_wider(id_cols = c(polygon_id, year),  names_from = c(variable, name))

continuous_summary <- covar_data|>
  filter(variable %in% continuous_vars)|>
  group_by(polygon_id, year, variable)|>
  summarise(
    mean = weighted.mean(value, coverage_fraction, na.rm = TRUE),
    max = max(value, na.rm = TRUE),
    sd = weighted_sd(value, coverage_fraction, na.rm = TRUE),
    median = matrixStats::weightedMedian(value, w = coverage_fraction, na.rm = TRUE),
    iqr = diff(Hmisc::wtd.quantile(value, weights = coverage_fraction, probs = c(0.25, 0.75), na.rm = TRUE)),
    .groups = 'drop'
  ) |> 
  pivot_longer(cols = -c(polygon_id, year, variable), values_to = "value") |> 
  pivot_wider(id_cols = c(polygon_id, year),  names_from = c(variable, name))

XData <- full_join(
  binary_summary,
  continuous_summary,
  by = c("polygon_id", "year")
)  

saveRDS(XData, "data/covariates/XData_hmsc.rds")
```


###  Phylogenetic Tree (phyloTree)

Note: We have to check if we have to resolve names used in the phylogeny database and the finish bird survey.

```{r}

spp_Y <- colnames(Y)

mctavish_mcc <- treeGet(version = "1.4", taxonomy_year = "2023")

mctavish_mcc$tip.label <- mctavish_mcc$tip.label |> 
  str_replace("_", " ")

not_in_mctavish <- !(spp_Y %in% mctavish_mcc$tip.label)

spp_Y_new <- spp_Y

for(i in 1:length(not_in_mctavish))(
  #i <- 5
  if(not_in_mctavish[i]){
    spp_Y.i <- spp_Y[i]
    to_rename <- taxize::synonyms(spp_Y.i, db = "itis")[[1]] |> 
      select(syn_name) |> 
      pull() |> 
      unique() |> 
      stringr::word(1, 2)
    spp_Y_new[i] <- to_rename
  }
)

# "Coloeus monedula" -> "Corvus monedula"
# "Phylloscopus sibillatrix" -> "Phylloscopus sibilatrix"  

phyloTree <- extractTree(species = spp_Y_new, version = 1.4)

phyloTree$tip.label <- spp_Y[match(str_replace(phyloTree$tip.label, "_", " ") , spp_Y_new)]

```


```{r}
#| label: fig-bird-tree
#| fig-width: 12
#| fig-height: 10
#| fig-cap: "Species phylogenetic tree"

plot(phyloTree, cex = 0.7, no.margin = TRUE)


```

## Species Trait Data (T)

```{r}

spp_Y_new <- data.frame("scientificName" = spp_Y_new)

TrData <- avonet |> 
  rename(scientificName = Species2) |>
  right_join(spp_Y_new, "scientificName") |>  
  select(scientificName, Trophic.Niche, Primary.Lifestyle, Migration) |> 
  as.data.frame()

TrData$scientificName <- spp_Y[match(TrData$scientificName, spp_Y_new$scientificName)]

row.names(TrData) <- TrData$scientificName 
 
TrData <- select(TrData, -scientificName)

```

## Set up the model

### Study desing

```{r}
studyDesign <- localityYear |> 
  select(karta, yr, sampleUnit) |> 
  mutate(across(everything(), as.factor)) |> 
  as.data.frame()

row.names(studyDesign) <- paste0(studyDesign$karta, ":", studyDesign$yr, ":" ,studyDesign$sampleUnit)

xy <- coordSampleUnits |> 
  select(-c(yr, punkt, sampleUnit)) |> 
  st_transform("EPSG:4326")|>  
    mutate(
    long = sf::st_coordinates(.)[,1],
    lat = sf::st_coordinates(.)[,2]
  ) |> 
  st_drop_geometry() |> 
  distinct() |> 
  as.data.frame()

le <-  levels(studyDesign$karta)
xy.karta <-  matrix(nrow=length(le), ncol=2)  
rownames(xy.karta) <-  le

for(i in 1:length(le)){
  xy.cle <- xy[which(xy$karta == le[i]), c("long", "lat")]
  xy.karta[le[i],] <- colMeans(xy.cle)
}

colnames(xy.karta) <-  c("long", "lat")

write.csv(xy.karta, "xy.karta.csv", row.names = T)

```

### Random effects

```{r}

rL.sampleUnit <- HmscRandomLevel(units = levels(studyDesign$sampleUnit))
rL.year <- HmscRandomLevel(units = levels(studyDesign$yr))

```

Clean Environment

```{r}
toKeep <- c("model", "modeltype","XData", "XFormula", "TrData", "TrFormula", "phyloTree", "Y", "rL.sampleUnit", "rL.locality", "rL.year", "studyDesign",
            "sampleUnits", "transform_Y_to_abunCpres", "xy.karta")
rm(list = setdiff(ls(), toKeep))
gc()

```

```{r}

colnames(XData)

XFormula <- ~ Klass + AGE_XX_P + BEECHVOL_XX_P + CONTORTAVOL_XX_P + BIRCHVOL_XX_P + DECIDUOUSVOL_XX_P + HEIGHT_XX_P + OAKVOL_XX_P + PINEVOL_XX_P + SPRUCEVOL_XX_P + TOTALVOL_XX_P + BIOMASS_XX_P + poly(wc2.1_30s_bio_5, degree = 2) + poly(wc2.1_30s_bio_18, degree = 2)


```


```{r}
TrFormula <- ~ log(Mass) + Habitat +  Trophic.Niche + Habitat.Density + Primary.Lifestyle + Migration

```

### Define model

#### Three random levels and full gausian process


```{r}
rL.locality <-  HmscRandomLevel(sData = xy.karta, longlat = TRUE)
```


```{r}
m.abu = Hmsc(Y = Y,
 XData = XData, XFormula = XFormula,
 TrData = TrData, TrFormula = TrFormula,
 phyloTree = phyloTree,
 distr = "lognormal poisson", 
 studyDesign = studyDesign, ranLevels = list("karta" = rL.locality, "yr" = rL.year, "sampleUnit" = rL.sampleUnit)
 )

m.abuCpres = Hmsc(Y = transform_Y_to_abunCpres(Y),
 XData = XData, XFormula = XFormula,
 TrData = TrData, TrFormula = TrFormula,
 phyloTree = phyloTree,
 distr = "lognormal poisson", 
 studyDesign = studyDesign, ranLevels = list("karta" = rL.locality, "yr" = rL.year, "sampleUnit" = rL.sampleUnit)
 )

m.pa = Hmsc(Y = 1*(Y>0),
 XData = XData, XFormula = XFormula,
 TrData = TrData, TrFormula = TrFormula,
 phyloTree = phyloTree,
 distr = "probit", 
 studyDesign = studyDesign, ranLevels = list("karta" = rL.locality, "yr" = rL.year, "sampleUnit" = rL.sampleUnit)
 )
```


```{r}
model <- "sbsF_full"
modeltype <- c("abu", "aCp", "pa")

dir.create("models", showWarnings = F)
models <-  list(m.abu, m.abuCpres, m.pa )
names(models) <- paste0(model, "_", modeltype)
save(models, file = file.path("models", "unfitted_models_full.RData"))
```

#### To make the process faster: spatial level and temporal level only and NNGP in GPU proccessing

```{r}
rL.locality <-  HmscRandomLevel(sData = xy.karta, longlat = TRUE, sMethod = 'NNGP', nNeighbours = 8)
```


```{r}
m.abu = Hmsc(Y = Y,
 XData = XData, XFormula = XFormula,
 TrData = TrData, TrFormula = TrFormula,
 phyloTree = phyloTree,
 distr = "lognormal poisson",
 studyDesign = studyDesign,
 ranLevels = list("karta" = rL.locality, "yr" = rL.year)
 )

m.abuCpres = Hmsc(Y = transform_Y_to_abunCpres(Y),
 XData = XData, XFormula = XFormula,
 TrData = TrData, TrFormula = TrFormula,
 phyloTree = phyloTree,
 distr = "lognormal poisson", 
 studyDesign = studyDesign, 
 ranLevels = list("karta" = rL.locality, "yr" = rL.year)
 )

m.pa = Hmsc(Y = 1*(Y>0),
 XData = XData, XFormula = XFormula,
 TrData = TrData, TrFormula = TrFormula,
 phyloTree = phyloTree,
 distr = "probit", 
 studyDesign = studyDesign, 
 ranLevels = list("karta" = rL.locality, "yr" = rL.year)
 )
```


```{r}
model <- "sbsF_ngpp_2rl"
modeltype <- c("abu", "aCp", "pa")

dir.create("models", showWarnings = F)
models <-  list(m.abu, m.abuCpres, m.pa)
names(models) <- paste0(model, "_", modeltype)
save(models, file = file.path("models", "unfitted_models_NGPP.RData"))
```

#### Three random levels: spatial, samling unit, and temporal level and NNGP in GPU proccessing

```{r}
rL.locality <-  HmscRandomLevel(sData = xy.karta, longlat = TRUE, sMethod = 'NNGP', nNeighbours = 8)
```


```{r}
m.abu = Hmsc(Y = Y,
 XData = XData, XFormula = XFormula,
 TrData = TrData, TrFormula = TrFormula,
 phyloTree = phyloTree,
 distr = "lognormal poisson", 
 studyDesign = studyDesign, ranLevels = list("karta" = rL.locality, "yr" = rL.year, "sampleUnit" = rL.sampleUnit)
 )

m.abuCpres = Hmsc(Y = transform_Y_to_abunCpres(Y),
 XData = XData, XFormula = XFormula,
 TrData = TrData, TrFormula = TrFormula,
 phyloTree = phyloTree,
 distr = "lognormal poisson", 
 studyDesign = studyDesign, ranLevels = list("karta" = rL.locality, "yr" = rL.year, "sampleUnit" = rL.sampleUnit)
 )

m.pa = Hmsc(Y = 1*(Y>0),
 XData = XData, XFormula = XFormula,
 TrData = TrData, TrFormula = TrFormula,
 phyloTree = phyloTree,
 distr = "probit", 
 studyDesign = studyDesign, ranLevels = list("karta" = rL.locality, "yr" = rL.year, "sampleUnit" = rL.sampleUnit)
 )


m.abu$call

```

```{r}
model <- "sbsF_ngpp_3rl"
modeltype <- c("abu", "aCp", "pa")

dir.create("models", showWarnings = F)
models <-  list(m.abu, m.abuCpres, m.pa)
names(models) <- paste0(model, "_", modeltype)
save(models, file = file.path("models", "unfitted_models_NGPP_3rl.RData"))
```


```{r}
sampleMcmc(m.abu, samples=2)
```

