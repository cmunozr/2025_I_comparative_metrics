---
title: "Quantitative biodiversity modelling for finance and companies"
subtitle: "Project 1: compare metrics"
author: 
  - name: "Carlos Muñoz"
    email: carlos_jair.munoz@cec.lu.se
    affiliations: 
      - ref: CEC

affiliations:
  - id: CEC
    name: Centre for Environmental and Climate Science, Lund University
    department: Centre for Environmental and Climate Science 
    address: Kontaktvägen 10
    city: Lund
    postal-code: 22362
    url: https://www.cec.lu.se/home
   
format:
  html:
    theme: cosmo # try other themes from https://quarto.org/docs/output-formats/html-themes.html
    toc: true # this will enable Table of Contents
    code-fold: false #try changing it and see what happens
    embed-resources: true # this is so your file would embed all images. 
    output-dir: "../quarto_reports/"  
  pdf: default
  docx: default

execute: 
  echo: true    #use to show/hide R-code in the report
  cache: false
number-sections: true #sectioner can be automatically numbered
title-block-banner: true
---

## Introduction

### General framework

-   Biodiversity is declining at an alarming rate, faster than ever in human history (Barnosky et al., 2011; Ceballos et al., 2015).
-   Economic activity is at the core of the biodiversity crisis, but biodiversity loss also represents a significant physical threat, reputational and regulatory risk to business and financial activities.
-   BIOPATH (Sweden) seeks to develop innovative approaches to integrate biodiversity considerations into financial and business decision-making to halt and reverse biodiversity loss.

### Biodiversity metrics

-   Biodiversity databases are the foundation for effective management strategies, but data alone isn't enough to stop biodiversity loss. We need to turn this data into useful information. This means using some way of biodiversity measurement to move activities from harming nature to being neutral or even positive.
-   Burgess and colleagues (2024) identified 573 metrics developed to inform decision-making related to biodiversity.
-   What metrics should be used?

#### A Tension

-   Business Perspective: Need for accessible, cost-effective and standardized. Importance of simplicity.
-   Natural scientific Perspective:Reflect ecological complexity.Capture multiple dimensions.
-   Maybe a set instead of only one silver bullet

### Challenge

-   The heart of the matter with biodiversity state metrics is how biodiversity change is interpreted and, consequently, how choices are made may be significantly impacted by the selection and response of biodiversity measures (Silvestro et al., 2023). It is critical to understand how different measures respond to changes, which metrics are particularly sensitive to early signals of biodiversity loss, and which metrics respond to changes consistently.
-   How transparently, efficiently, and credibly these measures capture the state and changes on multiple dimensions of biodiversity?

### Objective

To compare biodiversity metrics using quantitative models to assess how accurately they reflect biodiversity states and trends in different biodiversity change scenarios.

-   Birds as focal study taxon
-   Biodiversity change because change of habitat: forestry management scenarios

**THERE IS A NOTORIOUS GAP ABOUT WHY TO USE BIRDS AND WHY FOREST**

### Rol of Hmsc

An Hmsc model provides predictions at the species-community level. This enables the spatial representation of biodiversity to analyse and evaluate biodiversity metrics’ performance and variation through environmental and spatial gradients while counting for uncertainties.

## Setup

### Environment Configuration

```{r, setup}
knitr::opts_knit$set(root.dir = "c:/Users/Carlos Munoz/Documents/Ph.D/6_courses/2025_I_comparative_metrics/")
```

### Load Libraries

```{r}
#| message: false
#| warning: false

library(matrixStats)
library(Hmsc)#
library(tidyverse)
library(data.table)
library(terra)
library(sf)#
library(maps)#
library(ape)
library(taxize)#
library(remotes)
library(phytools)
library(openxlsx)
library(tidyterra)
library(viridis)
library(lwgeom)
library(ggrepel)
library(gt)
library(exactextractr) #
library(patchwork) #
library(dplyr)
library(units)
```

### Global Options & Helper Functions

```{r}
options("GDAL_DATA" = Sys.getenv("GDAL_DATA")) # Ensure GDAL_DATA is set if needed
options("OSR_USE_NON_DEPRECATED" = "NO")
```

```{r}
set.seed(11072024)
```


```{r}
transform_Y_to_abunCpres <- function(data) {
  data |> 
    mutate(across(everything(), ~ ifelse(. == 0, NA, scale(log(.)))))
}
```

```{r}
calculate_angle <- function(p1, p2, p3) {
  v1 <- p1 - p2
  v2 <- p3 - p2
  
  # Calculate the dot product
  dot_product <- sum(v1 * v2)
  
  # Calculate the magnitude (length) of each vector
  mag1 <- sqrt(sum(v1^2))
  mag2 <- sqrt(sum(v2^2))
  
  # The angle in radians is the arccosine of the dot product divided by the product of the magnitudes
  angle_rad <- acos(dot_product / (mag1 * mag2))
  
  # Convert radians to degrees
  angle_deg <- angle_rad * 180 / pi
  return(angle_deg)
}
```

```{r}
source("R/_utilities_transform_covariates.R")
```


## Data loading

### Load Finnish Bird Survey (FBS) Data


```{r}
# coordinates 
coords <- st_read("data/fbs/vakiolinja/Vakiolinjat_routes.geojson") |> 
  st_transform(2393)

# routes (names and dates)
routes <- read.csv("data/fbs/vakiolinja/vakiolinja1_20250626.csv")

# occurrences by route (spp key, and counts in points and lines)

occurr_route <- read.csv("data/fbs/vakiolinja/vakiolinja2_20250626.csv") 

# occurrences by section route (spp key, and counts in points and lines)

occurr_sect_route <- read.csv("data/fbs/vakiolinja/vakiolinja3_20250626.csv") 


```

### Load Auxiliary Files

Species list from Bakx et al, 2023.

```{r}
species_list <- read.xlsx(xlsxFile = "data/ecs24559-sup-0001-appendixs1.xlsx", sheet = 1)
```

Traits information
```{r}
avonet <- read.xlsx("data/traits/AVONET/TraitData/AVONET3_BirdTree.xlsx", sheet = 2)
```

Dictionary of processed covariates (Shultz)
```{r}
dict_covar <- read.csv("data/covariates/dictionary_covariates.csv", sep = ";")
```

Phylogenetic information

```{r}
birdTreeSpp <- read.csv("data/bird_tree/birds_tree_swedish/birdTree_spp.csv")

```

Multi Source Forest National Inventory available data years

```{r}
ms_nfi_years <- c(2009, 2011, 2013, 2015, 2017, 2019, 2021)
```


## FBS Data Preprocessing & Cleaning


```{r}
# Extracting and transforming route number information in order to use it as an easier index

coords_nm <- str_split(coords$name, pattern = ", ", simplify = T) |> 
  as.data.frame()
names(coords_nm) <- c("vakio", "str1", "str2", "numer")

coords <- bind_cols(coords, coords_nm) |>
  mutate(vakio = as.numeric(vakio), spatial_length = drop_units(st_length(coords))) |> 
  arrange(vakio)

routes <-  routes |> 
  mutate(vakio = as.numeric(vakio))
occurr_route <- occurr_route |> 
  mutate(vakio = as.numeric(vakio))
occurr_sect_route <- occurr_sect_route |> 
  mutate(vakio = as.numeric(vakio)) |> 
  rename(scientificName_old = scientificName)
  
```

### Taxonomical retrieving

Species information in the FBS routes. There is no taxonomic data, so it needs to be retrieved from external data sets (using the GBIF and the Integrated Taxonomic Information System, ITIS as backup)

```{r}
spp_in_route <- unique(occurr_sect_route$scientificName_old) |> sort()

run <-  F

if(run == T){
  tax_in_route_raw <- taxize::classification(spp_in_route, db = "gbif")
  missing_index <- which(is.na(tax_in_route_raw))
  missing <- spp_in_route[missing_index]
  tax__in_route_raw <- tax_in_route_raw[-missing_index]
  missing_tax <- taxize::classification(missing, db = "itis")
  tax_in_route_raw <- c(tax_in_route_raw, missing_tax)
  saveRDS(object = tax_in_route_raw, file = "data/fbs/tax_raw_data.rds")
  
  rm(missing_index, missing, missing_tax)
  
}else{
  
  tax_in_route_raw <- readRDS(file = "data/fbs/tax_raw_data.rds")
  
}

tax_in_route_list <- list()

for (current_name in names(tax_in_route_raw)) {
  
  current_data <- tax_in_route_raw[[current_name]]
  
  if (inherits(current_data, "data.frame")) {
    current_df <- as.data.frame(current_data)
    n.r <- nrow(current_df)
    cols <- current_df$rank
    high_rank <- cols[n.r]
    
    info <- as.list(current_df$name)
    names(info) <- cols
    info <- as.data.frame(info)
    
    info$id <- as.integer(current_df$id[n.r])
    info$input_name <- current_name 
    info$high_rank <- high_rank
    
    tax_in_route_list[[current_name]] <- info
  }else{
    message(paste("Species with name", current_name, "in route has status NULL or NA"))
  }
}

cols <- c("subkingdom", "infrakingdom", "subphylum", "infraphylum", "superclass",
          "subclass", "infraclass", "suborder", "superfamily")

tax_in_route <- list_rbind(tax_in_route_list) |> 
  relocate(subfamily, .after = family) |> 
  relocate(subspecies, .after = species) |> 
  select(-any_of(cols)) |> 
  rename(scientificName = species)

index_odo <- which(tax_in_route$subfamily == "Odocoileinae")
tax_in_route$kingdom[index_odo] <- "Animalia"
tax_in_route$phylum[index_odo] <- "Chordata"
tax_in_route$class[index_odo] <- "Mammalia"
tax_in_route$order[index_odo] <- "Artiodactila"
tax_in_route$family[index_odo] <- "Cervidae"

rm(index_odo, current_data, current_df, n.r, cols, high_rank, info, tax_in_route_raw, tax_in_route_list)

```


```{r}
occurr_sect_route <- tax_in_route |> 
  select(scientificName, input_name) |> 
  rename(scientificName_old = input_name) |> 
  right_join(occurr_sect_route, by = "scientificName_old")
```

```{r}
target_spp_in_route <- tax_in_route[!duplicated(tax_in_route$scientificName), ] |> 
  filter(high_rank == "species" | high_rank ==	"subspecies", class == "Aves") |> 
  filter(scientificName %in% species_list$Latin_name)
```

From the pool of species we have `r length(unique(tax_in_route$scientificName))` in the FBS routes. We are missing the next birds:

```{r}
species_list$Latin_name[!(species_list$Latin_name %in% target_spp_in_route$scientificName)]

```


## Exploratory Data Analysis & Quality Checks


### Create Master Event Table

```{r}

master_events <- routes |>
  # FLAG 1: Temporal Filter ---
  # Time window: late Spring/Summer routes of selected years ( because data availability of the multi source national forest inventory, MS-NFI)
  # Check vents outside the desired time window.
  mutate(
    is_valid_date = (month >= 5 & month <= 7 & year %in% ms_nfi_years) 
  ) |>
  # FLAG 2: Species Filter ---
  # Check which routes contain at least one observation of a target species.
  left_join(
    occurr_sect_route |>
      filter(scientificName %in% target_spp_in_route$scientificName) |> 
      distinct(vakio) |>
      mutate(has_target_species = TRUE),
    by = "vakio"
  ) |>
  #
  # FLAG 3: Biotope Data Quality ---
  # Check for routes that are missing biotope information in any given year.
  #
  left_join(
    occurr_sect_route |>
      group_by(year, vakio) |>
      summarise(
        sections_missing_biotope = sum(biotope == ""),
        total_sections = n(),
        .groups = 'drop'
      ) |>
      # A route is invalid if all its sections are missing biotope info
      mutate(has_no_biotope_data = (sections_missing_biotope == total_sections)) |>
      filter(has_no_biotope_data) |>
      distinct(vakio, .keep_all = TRUE), # Keep only the invalid routes to flag them
    by = c("vakio", "year")
  ) |>
  #
  # FLAG 4: Spatial Length Discrepancy ---
  # Check if the recorded line length differs from the spatial object length.
  left_join(coords, by = "vakio") |>
  mutate(
    length_difference = abs(linelength - spatial_length),
    is_length_discrepant = (length_difference > 50)
  ) |>
  #
  mutate(
    has_target_species = !is.na(has_target_species),
    has_no_biotope_data = !is.na(has_no_biotope_data)
  )
```

### Exploratory Analysis of Filter Criteria

How much data is affected by missing biotope information?

```{r}
# Summarize how many years each route is missing all biotope data
biotope_missing_summary <- master_events |>
  filter(has_no_biotope_data == TRUE) |>
  count(vakio, year, name = "times_missing")

# Join this summary to the spatial data for mapping
coords_for_plot <- coords |>
  left_join(biotope_missing_summary, by = "vakio") |>
  # If a route never had missing data, times_missing will be NA. We set it to 0.
  mutate(times_missing = as.factor(ifelse(is.na(times_missing), 0, times_missing)))

# What percentage of routes have this issue at least once?
# And how many times have they been without biotope info?
n_routes_total <- n_distinct(master_events$vakio)
n_routes_with_missing_data <- n_distinct(biotope_missing_summary$vakio)
percent_affected <- round((n_routes_with_missing_data / n_routes_total) * 100, 2)

print(paste0(percent_affected, "% of routes have at least one year with no biotope data."))

# Display the frequency table
table(coords_for_plot$times_missing) |>
  as.data.frame() |>
  rename(
    `Years of Missing Biotope Data` = Var1,
    `Number of Routes` = Freq
  ) |>
  gt()
```

How are the routes with missing biotope data distributed spatially?

```{r}
# mapping spreading of no biotope data
fin <- st_as_sf(map(database = "world", regions = "finland", plot = FALSE, fill = TRUE))

# Create a color palette
lvls <- levels(coords_for_plot$times_missing)
turbo_discrete_colors <- viridis_pal(option = "turbo")(n = length(lvls) - 1)
custom_colors <- c(
  "0" = "gray",
  setNames(turbo_discrete_colors, lvls[lvls != "0"])
)

# Plot the map
coords_for_plot |>
  ggplot() +
  geom_sf(data = fin) +
  geom_sf(aes(color = times_missing), size = 1.5) +
  scale_color_manual(
    values = custom_colors,
    name = "Number of Years Missing Data",
    drop = FALSE
  ) +
  labs(
    title = "Frequency of Years with Missing Biotope Data per Survey Route",
    subtitle = "Routes with no missing data are shown in grey."
  ) +
  theme_minimal()
```
How many routes are missing data each year?

```{r}
missing_by_year <- master_events |>
  group_by(year) |>
  summarise(
    # Count of routes missing data that year
    n_routes_missing = sum(has_no_biotope_data),
    # Total routes sampled that year
    n_routes_sampled = n(),
    # Percentage of routes missing data that year
    percent_missing = (n_routes_missing / n_routes_sampled) * 100
  )

# Breakdown of missing biotope data by year
missing_by_year |> 
  gt()

```

What is the spatial distribution of routes with target species?

```{r}
# Summarize at the route level: does a route EVER have a target species?
species_summary <- master_events |>
  group_by(vakio) |>
  summarise(has_target_species_ever = any(has_target_species))

# Join to spatial data for plotting
coords_for_species_plot <- coords |>
  left_join(species_summary, by = "vakio")

# Plot the map
coords_for_species_plot |>
  ggplot() +
  geom_sf(data = fin) +
  geom_sf(aes(color = has_target_species_ever), size = 1.5, alpha = 0.8) +
  scale_color_manual(
    values = c("TRUE" = "gray", "FALSE" = "red"),
    labels = c("TRUE" = "Species Information", "FALSE" = "No Species Information"),
    name = "Data Availability",
  ) +
  labs(
    title = "Spatial Distribution of Finnish Bird Survey Routes",
    subtitle = "Highlighting routes with and without focal species data"
  ) +
  theme_minimal()
```
If we remove routes with no biotope data, how much information do we lose of the target species?

```{r}
# Get the list of eventIDs that will be removed due to the biotope filter
events_to_remove <- master_events |>
  filter(has_no_biotope_data == TRUE) |>
  pull(eventId) |>
  unique()

# Flag occurrence data
loss_analysis_data <- occurr_sect_route |> 
  filter(scientificName %in% target_spp_in_route$scientificName) |>
  mutate(
    # Flag records based on whether their eventID is in the removal list
    biotope_logic = ifelse(eventId %in% events_to_remove, 0, 1)
  )

# --- summary ---
occurrence_data <-  table(scientificName = loss_analysis_data$scientificName,
                          biotope_logic = loss_analysis_data$biotope_logic) |>
  as.data.frame() |>
  pivot_wider(id_cols = scientificName,
              names_from = biotope_logic,
              values_from = Freq,
              names_prefix = "flag_") |>
  rename(occurrences_lost = flag_0,
         occurrences_kept = flag_1)

paircount_data <- loss_analysis_data |>
  group_by(scientificName, biotope_logic) |>
  summarise(total_pairCount = sum(pairCount, na.rm = TRUE), .groups = 'drop') |>
  pivot_wider(
    id_cols = scientificName,
    names_from = biotope_logic,
    values_from = total_pairCount,
    values_fill = 0,
    names_prefix = "flag_"
  ) |>
  rename(paircount_lost = flag_0,
         paircount_kept = flag_1)

# Join and display the final table
left_join(occurrence_data, paircount_data, by = "scientificName") |>
  mutate(
    total_occurrences = occurrences_lost + occurrences_kept,
    percent_occurrences_lost = round((occurrences_lost / total_occurrences) * 100, 2),
    total_paircount = paircount_lost + paircount_kept,
    percent_paircount_lost = round((paircount_lost / total_paircount) * 100, 2)
  ) |>
  arrange(desc(percent_paircount_lost)) |>
  gt() |>
    tab_header(title = "Potential Data Loss from Removing Routes with No Biotope Info")
  


```


### Apply Filter
```{r}
valid_events <- master_events |>
  filter(
    is_valid_date == TRUE,
    has_target_species == TRUE,
    has_no_biotope_data == FALSE,
    is_length_discrepant == FALSE
  )

# Extract the unique IDs for the valid events and routes
valid_event_ids <- unique(valid_events$eventId)
valid_vakio_ids <- unique(valid_events$vakio)

#
# --- DATA CLEANING ---

coords <- coords |>
  filter(vakio %in% valid_vakio_ids)

routes <- routes |>
  filter(eventId %in% valid_event_ids)

occurr_route <- occurr_route |>
  filter(eventId %in% valid_event_ids)

occurr_sect_route <- occurr_sect_route |>
  filter(
    eventId %in% valid_event_ids,
    scientificName %in% target_spp_in_route$scientificName
  )
```


```{r}
# Plot the map
coords |>
  ggplot() +
  geom_sf(data = fin) +
  geom_sf()
```

#### Species analysis


What is the distribution of bird orders among the target species after filtering?

```{r}
# The code is the same, but the input is now correctly focused
aves_order_data_barplot <- occurr_sect_route |> 
  select(scientificName) |> 
  distinct() |> 
  left_join(target_spp_in_route, by = "scientificName") |>
  filter(class == 'Aves') |>
  dplyr::count(order, name = 'n') |>
  arrange(desc(n))

ggplot(aves_order_data_barplot, aes(x = reorder(order, -n), y = n, fill = order)) +
  geom_col(color = 'black', show.legend = FALSE) +
  labs(title = 'Distribution of Orders Among Target Bird Species',
       x = 'Order',
       y = 'Number of Species') +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(hjust = 0.5, size = 14, face = 'bold')) +
  geom_text(aes(label = n), vjust = -0.5, size = 3.5)
```

Which Target Species Were Lost After Filtering?

```{r}
# Get a unique list of species present in the final, filtered dataset
species_in_final_dataset <- unique(occurr_sect_route$scientificName)

target_species <- unique(target_spp_in_route$scientificName)

# Compare original target list ('species') with the final list
lost_species <- setdiff(target_species, species_in_final_dataset)

# Display the result in a table
if (length(lost_species) > 0) {
  tibble(Lost_Target_Species = lost_species) |>
    gt() |>
    tab_header(
      title = "Target Species Removed by Data Quality Filters"
    )
}
```


## Feature engineer

### Create Biotope Polygon Buffers

```{r}
biotope_polygon_L3_log <- tidyr::expand_grid(
  vakio = coords$vakio,
  year = ms_nfi_years
) |>
  mutate(
    path = NA_character_,
    was_processed = FALSE,
    was_extended = FALSE,
    extension_distance_m = NA_real_,
    point_strips = list(NULL)
  ) |>
  arrange(year, vakio)

for (y in 1:length(ms_nfi_years)) {
  #y <- 7
  current_year <- ms_nfi_years[y]
  message(paste("--- Processing year:", current_year, "---"))

  all_buffers_for_year <- list()

  for (a in 1:nrow(coords)) {
    # 
    route_id <- coords$vakio[a]
    
    log_row_index <- which(biotope_polygon_L3_log$vakio == route_id & biotope_polygon_L3_log$year == current_year)
    
    route_a <- coords |>
      filter(vakio == route_id) |> 
      st_simplify(preserveTopology = T, dTolerance = 20) #<- this solve spicules and a lot of problems but get into newer ones

    biotope_route_a <- occurr_sect_route |>
      filter(year == current_year, vakio == route_id) |>
      select(biotopeStripId, startMeters, endMeters, meters, biotope, biotopeSpecifier) |>
      distinct() |> 
      arrange(startMeters) |> 
      mutate(
        r_startMeters = startMeters / as.numeric(st_length(route_a)),
        r_endMeters = endMeters / as.numeric(st_length(route_a))
      )
    
    if(any(biotope_route_a$r_startMeters > 1)){
      d <- (biotope_route_a$r_endMeters[nrow(biotope_route_a)] - 1) * as.numeric(st_length(route_a))
      route_a <- st_extend_line(line = route_a, distance = d, end = "end")
      
      biotope_route_a <- biotope_route_a |>
        mutate(
          r_startMeters = startMeters / as.numeric(st_length(route_a)),
          r_endMeters = endMeters / as.numeric(st_length(route_a))
        )
      
      biotope_polygon_L3_log$was_extended[log_row_index] <- TRUE
      biotope_polygon_L3_log$extension_distance_m[log_row_index] <- d
    
    }else{
      biotope_polygon_L3_log$was_extended[log_row_index] <- FALSE
    }
    
    if (nrow(biotope_route_a) > 1) {
      
      biotope_polygon_L3_log$was_processed[log_row_index] <- TRUE
      
      route_buffers <- list()
      for (r in 1:nrow(biotope_route_a)) {
        # r <- 30
        equal <- biotope_route_a[r,c("r_startMeters", "r_endMeters")] |> 
          unlist() |> 
          diff()
        
        if(equal != 0){
          
        # Create the line segment for this biotope section
        line_section <- st_linesubstring(
          route_a,
          from = biotope_route_a$r_startMeters[r],
          to = biotope_route_a$r_endMeters[r]
        )

        # A standard buffer on the raw, angular route lines can create unrealistic polygon artifacts around sharp corners.
        # To solve this, we split each section and create sub-buffers and then join them.
        
        segments <- nngeo::st_segments(line_section, progress = F)
       
        segment_buffers <- st_buffer(segments, dist = 150, endCapStyle = "FLAT")
        
        final_buffer <- st_union(segment_buffers)
        
        final_buffer_sf <- st_sf(geometry = st_sfc(final_buffer), crs = st_crs(route_a)) |>
            bind_cols(biotope_route_a[r,])

        route_buffers[[r]] <- final_buffer_sf
        
        }else{
          
          value_in_log <- biotope_polygon_L3_log$point_strips[log_row_index] |> unlist()
          
          point_strip_r <-  biotope_route_a$biotopeStripId[r]
          
          warning(paste(point_strip_r, "route", route_id, "year", current_year, "is a point observation, check log."))
          
          value_to_log <- compact(list(c(value_in_log, point_strip_r)))
          
          biotope_polygon_L3_log$point_strips[[log_row_index]] <- value_to_log
          
        }
        
      }
      all_buffers_for_year[[a]] <- do.call(rbind, route_buffers)
    }
  }

  # Combine and save the final file for the year
  valid_buffers <- all_buffers_for_year[!sapply(all_buffers_for_year, is.null)]
  year_sf <- do.call(rbind, valid_buffers)

  output_filename <- paste0("data/fbs/route_sections_L3_", current_year, ".gpkg")
  write_sf(year_sf, output_filename, delete_layer = TRUE)
  biotope_polygon_L3_log[biotope_polygon_L3_log$year == current_year, "path"] <- output_filename
  message(paste("Saved file:", output_filename))

}

saveRDS(biotope_polygon_L3_log, file.path("data", "fbs", "biotope_polygon_L3_log.rds"))

biotope_polygon_L3_log <- readRDS(file.path("data", "fbs", "biotope_polygon_L3_log.rds"))
```


```{r}
rm(route_buffers, line_section, segments, segment_buffers, final_buffer, final_buffer_sf, 
   route_buffers, all_buffers_for_year, valid_buffers, year_sf, output_filename)
```

```{r}
l3_polygon_paths <- output_paths$path[!is.na(output_paths$path)]
l2_output_paths <- data.frame("year" = ms_nfi_years, "path_L2" = NA)

for (i in 1:length(l3_polygon_paths)) {
  
  l3_file <- l3_polygon_paths[i]
  current_year <- as.numeric(gsub("[^0-9]", "", basename(l3_file)))
  message(paste("Processing:", basename(l3_file)))
  
  # Read the detailed Level 3 polygons
  polygons_l3 <- st_read(l3_file)
  
  # Dissolve the geometries
  # We group by the route ID (vakio) and the Level 2 biotope code
  polygons_l2 <- polygons_l3 |>
    group_by(vakio, biotope) |>
    # The key step: st_union() merges all geometries within each group
    summarise(
      .groups = "drop" # Drop the grouping
    )
  
  # Save the new, dissolved polygons to a new file
  output_filename_l2 <- paste0("data/fbs/route_sections_L2_", current_year, ".gpkg")
  write_sf(polygons_l2, output_filename_l2, delete_layer = TRUE)
  l2_output_paths$path_L2[l2_output_paths$year == current_year] <- output_filename_l2
  message(paste("Saved dissolved file:", output_filename_l2))
}
```



### Process Environmental Covariates

Online repositories are not deployed as Web Feature Service in which we can work "in-situ". It was needed to download the data sets. Data sets are massive and splitted over several storage units. To consult how they were downloaded please refer to scripts "_utilities_download_covariates.R" and "_utilities_download_covariates.py". Dowloaded files were processing following the "_utilities_fetch_covariates.R" script.

Raw data of each route with a 150m buffer and from metso/non metso stands is extracted with the script "_utilities_fetch_covariates.R". Once with this information 
we can compare each type of unit to inform potential risk of exrapolation. We sample 10000 stands for each type (metso/non metso) and all the route buffers.

```{r}

covar_nm <- dict_covar |> 
  pull(var_new) |> 
  unique()

year_route_sampling <- occurr_sect_route |> 
  group_by(year, vakio) |>
  summarise(total_sections = n(), .groups = 'drop')

```

```{r}

folder_name <- file.path("data", "intermediate_covariates") 


run <- F

if(run){
  
  dir.create(folder_name, showWarnings = FALSE)
  output_rds_paths <- as.character()
  
  metso_stands_df <- st_read("data/metso/treatment_control_stand.gpkg") |> 
  st_drop_geometry() |> 
  select(standid, metso)
  
  sampled_stand_ids <- metso_stands_df |> 
  group_by(metso) |>
  sample_n(size = 10000, replace = FALSE) |>
  ungroup() |>
  pull(standid)
  
  rds_data_dir <- file.path("D:","intermediate_aggregated")

  rds_files <- list.files(rds_data_dir, pattern = "\\.rds$", recursive = TRUE, full.names = TRUE)

  for (i in 1:length(covar_nm)) {
    
    covariate <-  covar_nm[i]
    
    message(paste("--- Processing covariate:", covariate, "---"))
  
    filtered_data <- filter_covariate_data(
      covariate_name = covariate,
      files = rds_files,
      sampled_ids = sampled_stand_ids,
      route_map = year_route_sampling
    )
    
    if(is.null(filtered_data)) next
  
    # writing a filtered data set just for coords because metso is a sample
    output_rds_path <- file.path(folder_name, paste0("filtered_cov_coords_", covariate, ".rds"))
    saveRDS(filter(filtered_data, polygon_source == "coords"), file = output_rds_path) 
    output_rds_paths[i] <- output_rds_path
    
    plot_data <- prepare_covariates_data_toplot(
      filtered_data = filtered_data,
      metso_map = metso_stands_df
    )
    
    is_binary <- dict_covar$binary[i]
    
    if (is_binary) {
      generate_binary_plot(covariate, plot_data, folder_name)
    } else {
      generate_covariate_plot(covariate, plot_data, folder_name)
    }
  }
  
}else{
  output_rds_paths <- list.files(folder_name, pattern = ".rds$", recursive = T, full.names = T)
}

```

## Model Input Assembly

### Species Occurrence Matrix (Y)

```{r}
Y <- occurrSppCoord |> 
  select(karta, punkt, yr, latin, count) |>
  pivot_wider(id_cols = c(karta, punkt, yr), names_from = latin, values_from = count,
              values_fn = sum, values_fill = 0) |> 
  select(-yr, -punkt, -karta) |> 
  as.data.frame()

Y <- Y[, order(colnames(Y))]

```

```{r}
P <-  colMeans(Y > 0)
range(P) |> round(4)
hist(P, xlab = "Species prevalence (P)",xlim=c(0,1))
```

Most species exhibit low prevalence, clustering towards the rare end of the spectrum, while a few species show a significantly higher prevalence, indicating they are common across the sampled areas, even an species is occurring in the `r max(P)` of sites (`r names(which(P == max(P)))`).

### XData

Now we are going to derive polygon-Level statistics from the raw values of the covariates.  We use the covariate data dictionary to filter each variable as either continuous or binary. Next, the two variable types are handled differently: for each polygon and year, continuous variables are summarized with a weighted mean and standard deviation, min and max, while binary variables are summarized by calculating the weighted proportion of '1's. 

```{r}
covar_data <- purrr::map_dfr(.x = output_rds_paths, .f = ~ compact(readRDS(.x))) |>
  select(-c(poly_id, dataset, polygon_source)) 

binary_vars <- dict_covar |> 
  filter(binary == 1) |> 
  pull(var_new)

continuous_vars <- dict_covar|>
  filter(binary == 0)|>
  pull(var_new)

binary_summary <- covar_data|>
  filter(variable %in% binary_vars)|>
  group_by(polygon_id, year, variable)|>
  summarise(
    prop = weighted.mean(value, coverage_fraction, na.rm = TRUE),
    count = ceiling(sum(coverage_fraction[value == 1], na.rm = TRUE)),
    .groups = 'drop'
  ) |> 
  pivot_longer(cols = -c(polygon_id, year, variable), values_to = "value") |> 
  pivot_wider(id_cols = c(polygon_id, year),  names_from = c(variable, name))

continuous_summary <- covar_data|>
  filter(variable %in% continuous_vars)|>
  group_by(polygon_id, year, variable)|>
  summarise(
    mean = weighted.mean(value, coverage_fraction, na.rm = TRUE),
    max = max(value, na.rm = TRUE),
    sd = weighted_sd(value, coverage_fraction, na.rm = TRUE),
    median = matrixStats::weightedMedian(value, w = coverage_fraction, na.rm = TRUE),
    iqr = diff(Hmisc::wtd.quantile(value, weights = coverage_fraction, probs = c(0.25, 0.75), na.rm = TRUE)),
    .groups = 'drop'
  ) |> 
  pivot_longer(cols = -c(polygon_id, year, variable), values_to = "value") |> 
  pivot_wider(id_cols = c(polygon_id, year),  names_from = c(variable, name))

XData <- full_join(
  binary_summary,
  continuous_summary,
  by = c("polygon_id", "year")
)  

saveRDS(XData, "data/covariates/XData_hmsc.rds")
```


###  Phylogenetic Tree (phyloTree)

First, we have to resolve names used in the phylogeny database and the bird survey.(BirdTree database)\[https://birdtree.org/\] The database come froms: (Jetz, W., G. H. Thomas, J. B. Joy, K. Hartmann, and A. O. Mooers. 2012. The global diversity of birds in space and time. Nature 491:444-448)\[https://www-nature-com.ludwig.lub.lu.se/articles/nature11631\]. There is no API, so write and retrieve manually. The species list from the database were web scrapping.

```{r}

sppY <- colnames(Y)

sppResolved <- data.frame("scientificName" = NA, "bird_tree" = NA)

run <- F

if(run){

  for(i in 1:length(sppY)){
    spp.i <- sppY[i]
    
    isMissingSppTree <- !(spp.i %in% birdTreeSpp$species)
      
    if(isMissingSppTree){
        
      if(spp.i == "Corvus cornix"){
        
        synSppY.i <- "Corvus corone"
          
        }else if(spp.i == "Loxia bifasciata"){
          synSppY.i <- "Loxia leucoptera"
    
        }else if(spp.i == "Curruca cantillans"){
          synSppY.i <- "Sylvia cantillans"
    
        }else if(spp.i == "Columba livia f. domestica"){
          synSppY.i <- "Columba livia"
          
        }else{
          
        synSppY.i <- synonyms(spp.i, db = "nbn", rec_only = T, accepted = F, rank = "species", ask = F) |> 
          map_df(~ .x)
        
        if(ncol(synSppY.i) == 1 | ncol(synSppY.i) == 0){
          synSppY.i <- NA
        }else{
          synSppY.i <-  synSppY.i |> 
            select(nameString) |> 
            pull()
          synSppY.i <- (synSppY.i[synSppY.i %in% birdTreeSpp$species]) |> 
            unique()
        }
    
        }
      }else{
        synSppY.i <- spp.i
      }
     sppResolved[i,"scientificName"] <- spp.i
     sppResolved[i,"bird_tree"] <- synSppY.i
  }
  
  write.csv(sppResolved, "data/bird_tree/taxonomical_synonyms_birdtree.csv", row.names = F)
}else{
  
  sppResolved <- read.csv("data/bird_tree/taxonomical_synonyms_birdtree.csv")

}


Trees <- ape::read.nexus("data/bird_tree/n100_naRemoved/output.nex")

## MISSING. Consensus method (?)


phyloTree <- Trees[[sample(1:100, 1)]]


phyloTree$tip.label <- phyloTree$tip.label |> 
  str_replace(pattern = "_", " ")

#----------------
# remove spp
# phyloTree <- drop.tip(phyloTree, c("Anser_indicus", "Falco_vespertinus"))

#----------------

phyloTree$tip.label <- sppResolved$scientificName[match(phyloTree$tip.label, sppResolved$bird_tree)]

```

## Species Trait Data (T)

```{r}
avonet <- avonet |> 
  rename(bird_tree = Species3) |>
  right_join(sppResolved, by = "bird_tree" ) |>  
  select(scientificName, Mass, Habitat, Habitat.Density, Trophic.Niche, Primary.Lifestyle, Migration)
 
TrData <- avonet |>  
  as.data.frame()

row.names(TrData) <- TrData$scientificName 
 
TrData <- select(TrData, -scientificName)

TrData <- TrData[order(row.names(TrData)), ]

```

## Set up the model

### Study desing

```{r}
studyDesign <- localityYear |> 
  select(karta, yr, sampleUnit) |> 
  mutate(across(everything(), as.factor)) |> 
  as.data.frame()

row.names(studyDesign) <- paste0(studyDesign$karta, ":", studyDesign$yr, ":" ,studyDesign$sampleUnit)

xy <- coordSampleUnits |> 
  select(-c(yr, punkt, sampleUnit)) |> 
  st_transform("EPSG:4326")|>  
    mutate(
    long = sf::st_coordinates(.)[,1],
    lat = sf::st_coordinates(.)[,2]
  ) |> 
  st_drop_geometry() |> 
  distinct() |> 
  as.data.frame()

le <-  levels(studyDesign$karta)
xy.karta <-  matrix(nrow=length(le), ncol=2)  
rownames(xy.karta) <-  le

for(i in 1:length(le)){
  xy.cle <- xy[which(xy$karta == le[i]), c("long", "lat")]
  xy.karta[le[i],] <- colMeans(xy.cle)
}

colnames(xy.karta) <-  c("long", "lat")

write.csv(xy.karta, "xy.karta.csv", row.names = T)

```

### Random effects

```{r}

rL.sampleUnit <- HmscRandomLevel(units = levels(studyDesign$sampleUnit))
rL.year <- HmscRandomLevel(units = levels(studyDesign$yr))

```

Clean Environment

```{r}
toKeep <- c("model", "modeltype","XData", "XFormula", "TrData", "TrFormula", "phyloTree", "Y", "rL.sampleUnit", "rL.locality", "rL.year", "studyDesign",
            "sampleUnits", "transform_Y_to_abunCpres", "xy.karta")
rm(list = setdiff(ls(), toKeep))
gc()

```

```{r}

colnames(XData)

XFormula <- ~ Klass + AGE_XX_P + BEECHVOL_XX_P + CONTORTAVOL_XX_P + BIRCHVOL_XX_P + DECIDUOUSVOL_XX_P + HEIGHT_XX_P + OAKVOL_XX_P + PINEVOL_XX_P + SPRUCEVOL_XX_P + TOTALVOL_XX_P + BIOMASS_XX_P + poly(wc2.1_30s_bio_5, degree = 2) + poly(wc2.1_30s_bio_18, degree = 2)


```


```{r}
TrFormula <- ~ log(Mass) + Habitat +  Trophic.Niche + Habitat.Density + Primary.Lifestyle + Migration

```

### Define model

#### Three random levels and full gausian process


```{r}
rL.locality <-  HmscRandomLevel(sData = xy.karta, longlat = TRUE)
```


```{r}
m.abu = Hmsc(Y = Y,
 XData = XData, XFormula = XFormula,
 TrData = TrData, TrFormula = TrFormula,
 phyloTree = phyloTree,
 distr = "lognormal poisson", 
 studyDesign = studyDesign, ranLevels = list("karta" = rL.locality, "yr" = rL.year, "sampleUnit" = rL.sampleUnit)
 )

m.abuCpres = Hmsc(Y = transform_Y_to_abunCpres(Y),
 XData = XData, XFormula = XFormula,
 TrData = TrData, TrFormula = TrFormula,
 phyloTree = phyloTree,
 distr = "lognormal poisson", 
 studyDesign = studyDesign, ranLevels = list("karta" = rL.locality, "yr" = rL.year, "sampleUnit" = rL.sampleUnit)
 )

m.pa = Hmsc(Y = 1*(Y>0),
 XData = XData, XFormula = XFormula,
 TrData = TrData, TrFormula = TrFormula,
 phyloTree = phyloTree,
 distr = "probit", 
 studyDesign = studyDesign, ranLevels = list("karta" = rL.locality, "yr" = rL.year, "sampleUnit" = rL.sampleUnit)
 )
```


```{r}
model <- "sbsF_full"
modeltype <- c("abu", "aCp", "pa")

dir.create("models", showWarnings = F)
models <-  list(m.abu, m.abuCpres, m.pa )
names(models) <- paste0(model, "_", modeltype)
save(models, file = file.path("models", "unfitted_models_full.RData"))
```

#### To make the process faster: spatial level and temporal level only and NNGP in GPU proccessing

```{r}
rL.locality <-  HmscRandomLevel(sData = xy.karta, longlat = TRUE, sMethod = 'NNGP', nNeighbours = 8)
```


```{r}
m.abu = Hmsc(Y = Y,
 XData = XData, XFormula = XFormula,
 TrData = TrData, TrFormula = TrFormula,
 phyloTree = phyloTree,
 distr = "lognormal poisson",
 studyDesign = studyDesign,
 ranLevels = list("karta" = rL.locality, "yr" = rL.year)
 )

m.abuCpres = Hmsc(Y = transform_Y_to_abunCpres(Y),
 XData = XData, XFormula = XFormula,
 TrData = TrData, TrFormula = TrFormula,
 phyloTree = phyloTree,
 distr = "lognormal poisson", 
 studyDesign = studyDesign, 
 ranLevels = list("karta" = rL.locality, "yr" = rL.year)
 )

m.pa = Hmsc(Y = 1*(Y>0),
 XData = XData, XFormula = XFormula,
 TrData = TrData, TrFormula = TrFormula,
 phyloTree = phyloTree,
 distr = "probit", 
 studyDesign = studyDesign, 
 ranLevels = list("karta" = rL.locality, "yr" = rL.year)
 )
```


```{r}
model <- "sbsF_ngpp_2rl"
modeltype <- c("abu", "aCp", "pa")

dir.create("models", showWarnings = F)
models <-  list(m.abu, m.abuCpres, m.pa)
names(models) <- paste0(model, "_", modeltype)
save(models, file = file.path("models", "unfitted_models_NGPP.RData"))
```

#### Three random levels: spatial, samling unit, and temporal level and NNGP in GPU proccessing

```{r}
rL.locality <-  HmscRandomLevel(sData = xy.karta, longlat = TRUE, sMethod = 'NNGP', nNeighbours = 8)
```


```{r}
m.abu = Hmsc(Y = Y,
 XData = XData, XFormula = XFormula,
 TrData = TrData, TrFormula = TrFormula,
 phyloTree = phyloTree,
 distr = "lognormal poisson", 
 studyDesign = studyDesign, ranLevels = list("karta" = rL.locality, "yr" = rL.year, "sampleUnit" = rL.sampleUnit)
 )

m.abuCpres = Hmsc(Y = transform_Y_to_abunCpres(Y),
 XData = XData, XFormula = XFormula,
 TrData = TrData, TrFormula = TrFormula,
 phyloTree = phyloTree,
 distr = "lognormal poisson", 
 studyDesign = studyDesign, ranLevels = list("karta" = rL.locality, "yr" = rL.year, "sampleUnit" = rL.sampleUnit)
 )

m.pa = Hmsc(Y = 1*(Y>0),
 XData = XData, XFormula = XFormula,
 TrData = TrData, TrFormula = TrFormula,
 phyloTree = phyloTree,
 distr = "probit", 
 studyDesign = studyDesign, ranLevels = list("karta" = rL.locality, "yr" = rL.year, "sampleUnit" = rL.sampleUnit)
 )


m.abu$call

```

```{r}
model <- "sbsF_ngpp_3rl"
modeltype <- c("abu", "aCp", "pa")

dir.create("models", showWarnings = F)
models <-  list(m.abu, m.abuCpres, m.pa)
names(models) <- paste0(model, "_", modeltype)
save(models, file = file.path("models", "unfitted_models_NGPP_3rl.RData"))
```


```{r}
sampleMcmc(m.abu, samples=2)
```

```{r}
# Yx <- Y |> 
#   pivot_longer(
#     cols = everything(), # Pivotear todas las columnas excepto "id"
#     names_to = "especie",
#     values_to = "abundancia"
#   ) |> 
#   filter(abundancia != 0) |> 
```

